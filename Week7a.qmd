---
title: "Week 7 - Evaluating Classification Models"
format: pdf
---

## Introduction:

Welcome to this lecture on the various methods used to evaluate classification models.
Evaluating the performance of a classification model is crucial for assessing its accuracy and effectiveness. In this lecture, we will explore several key evaluation metrics and techniques used in the field of machine learning.

## Importance of Model Evaluation:

Model evaluation helps us understand how well a classification model is performing. It provides insights into a model's strengths and weaknesses. Evaluation metrics guide model selection, hyperparameter tuning, and ultimately, decision-making in real-world applications.

## Common Evaluation Metrics:

There are several common evaluation metrics used for classification models:

1. Accuracy: Accuracy is the most straightforward metric, measuring the ratio of correctly predicted instances to the total number of instances. It is suitable when classes are balanced, but it may not be informative for imbalanced datasets.

2. Precision: Precision measures the ratio of true positives (correctly predicted positive instances) to the total predicted positive instances. It focuses on the accuracy of positive predictions and is useful when minimizing false positives is essential.

3. Recall (Sensitivity or True Positive Rate): Recall measures the ratio of true positives to the total actual positive instances. It assesses the model's ability to identify all positive instances and is essential when minimizing false negatives is critical.

4. Confusion Matrix: A confusion matrix provides a detailed breakdown of a model's predictions. It includes counts of true positives, true negatives, false positives, and false negatives.

## Errors that can occur and implications

Type 1 Error (False Positive)

A Type 1 error occurs when the hypothesis test incorrectly rejects a true null hypothesis. This is also known as a "false positive" result. In other words, the test indicates that there is an effect or a difference when, in fact, there isn't one. The probability of making a Type 1 error is denoted by the alpha level (α), which is set by the researcher before conducting the test. A common alpha level is 0.05, indicating a 5% risk of rejecting the null hypothesis when it is actually true.
Example:

In a drug trial, a Type 1 error would occur if we conclude that a new medication is effective in treating a condition when it actually has no effect. This would lead to falsely believing in the efficacy of the drug.
Type 2 Error (False Negative)

A Type 2 error occurs when the hypothesis test fails to reject a false null hypothesis. This is known as a "false negative" result. In this case, the test suggests that there is no effect or difference when, in reality, there is one. The probability of making a Type 2 error is denoted by the beta level (β), and its complement (1-β) is known as the power of the test, which reflects the test's ability to correctly reject a false null hypothesis.
Example:

In the context of the same drug trial, a Type 2 error would happen if we conclude that the new medication is not effective in treating a condition when it actually is. This would result in overlooking a potentially beneficial treatment.
Balancing Type 1 and Type 2 Errors

In practice, there's often a trade-off between minimizing Type 1 and Type 2 errors. Decreasing the risk of one typically increases the risk of the other. The choice of alpha level affects this balance; a lower alpha level reduces the risk of a Type 1 error but may increase the risk of a Type 2 error, and vice versa. The optimal balance depends on the specific context and the consequences of each type of error. For instance, in medical testing, it might be preferable to accept a higher risk of Type 1 errors (falsely identifying a condition) to reduce the risk of Type 2 errors (missing a diagnosis), given the potential health implications.

## Methods of Evaluation:

Model evaluation involves various techniques to assess a classification model's performance:

1. Cross-Validation:

Cross-validation (e.g., k-fold cross-validation) splits the dataset into multiple subsets (folds) for training and testing. It helps estimate a model's performance on unseen data and reduces overfitting.

2. Train-Test Split:

A common approach is to split the dataset into a training set and a testing set. The model is trained on the training set and evaluated on the testing set.

3. Stratified Sampling:

When dealing with imbalanced datasets, stratified sampling ensures that the training and testing sets maintain the same class distribution.

4. Leave-One-Out Cross-Validation (LOOCV):

LOOCV is a special case of cross-validation where each data point is treated as a test sample once, and the model is trained on the remaining data. It is useful for small datasets but can be computationally expensive.

## Conclusion:

Evaluating classification models is a critical step in the machine learning workflow.
Various metrics, such as accuracy, precision, recall, F1-score, ROC curves, and confusion matrices, provide insights into model performance. Techniques like cross-validation, train-test splits, and stratified sampling help ensure robust evaluation.
