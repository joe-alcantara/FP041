---
title: "Week 9 - Evaluation of Regression Models"
format: pdf
---

## Introduction:

Welcome to this unit on methods of evaluating regression models. Regression models are essential in mathematical modeling and data analysis, and their performance needs to be assessed rigorously. Without proper evaluation, researchers may misinterpret the relationships between variables. They might erroneously conclude that certain predictors have a significant impact on the outcome when they do not or vice versa. Inadequately evaluated regression models may produce inaccurate predictions. If the model does not capture the underlying relationships between variables effectively, its predictive performance may be poor, leading to unreliable forecasts or estimates. In this unit, we will explore various evaluation methods and metrics used to assess the quality and accuracy of regression models.

## Importance of Model Evaluation:

Model evaluation is critical to determine how well a regression model fits the data and makes predictions. It helps in selecting the best model among competing models and guides model refinement. Accurate model evaluation is essential for making informed decisions in various fields, including economics, engineering, and the sciences.

## Common Evaluation Metrics:

Several metrics are commonly used to evaluate regression models:

1. Mean Squared Error (MSE):

MSE measures the average squared difference between predicted and actual values. It penalizes larger errors more than smaller ones, making it sensitive to outliers.
The formula for MSE is: 

$$MSE = \frac{\sum(y_i- \bar{y})^2}{n}$$

where $y_i$ is the actual value, $\bar{y}$ is the mean of actual values, and $n$ is the number of data points.

2. Root Mean Squared Error (RMSE):

RMSE is the square root of MSE and is in the same units as the dependent variable. It provides a more interpretable measure of error.

$$RMSE = \sqrt{MSE}$$
3. Mean Absolute Error (MAE):

MAE measures the average absolute difference between predicted and actual values. It is less sensitive to outliers compared to MSE.

$$MAE = \frac{\sum{|y_i - \bar{y}|}}{n} $$

where ŷ is the predicted value.

4. Multiple R-squared (R^2):

R² represents the proportion of variance in the dependent variable explained by the model. It ranges from 0 to 1, where a higher value indicates a better fit. For this we need to calculate two terms. 

*Sum of Squared Errors*

The sum of squares error (SSE) or residual sum of squares (RSS, where residual means remaining or unexplained) is the difference between the observed and predicted values.

$$SSE = \sum{\epsilon_i^2}$$

where \epsilon is the difference between the acutal value of the dependent variable and the predicted value.

$$\epsilon_i = y_i - \hat{y}$$

*Sum of Squares Total*
The sum of squares total (SST) or the total sum of squares (TSS) is the sum of squared differences between the observed dependent variables and the overall mean. 

$$SST = \sum{(y_i - \bar{y})^2}$$
Where $y_i$ is the observed dependant value and $\bar{y}$ is the mean of the dependent variable.

From these two variables, you are able to calculate Multiple R-squared.

$$R^2 = 1 - (SSE / SST)$$

where SSE is the sum of squared errors and SST is the total sum of squares.

5. Adjusted R-squared (Adjusted R²):

Adjusted R² adjusts R² for the number of predictors in the model. It penalizes the addition of unnecessary variables.

$$Adjusted R^2 = 1 - [(1 - R²) * ((n - 1) / (n - p - 1))]$$

where n is the number of data points and p is the number of predictors.

## Exercise

Let's assume we have the following dataset representing the relationship between the number of hours studied and exam scores for a group of students:

| Hours Studied (x) |	Exam Score (y) |
|---|----|
| 2 |	60 |
| 3 |	70 |
| 4	| 80 |
| 5 |	85 |
| 6 |	90 |

We want to build a simple linear regression model to predict exam scores based on the number of hours studied.

1. Calculate the regression line equation:

Using the formulas:

$$m=\frac{n(\sum{xy}−(\sum{x})(\sum{y}))}{n(\sum{x^2})−(\sum{x})^2}$$
$$b=\frac{\sum{y}=m(\sum{x})}{n}$$
  
Where:
- $=5$
- $\sum{x}=20$
- $\sum{y}=385$
- $\sum{xy}=1350$
- $\sum{x^2}=70$

Calculating:

$$m=\frac{5(1350)−(20)(385)}{5(70)−(20)^2}=\frac{6750−7700}{350−400}=\frac{−950}{−50}=19$$

$$b=\frac{385−19(20)}{5}=\frac{385−380}{5}=\frac{5}{5}=1$$
So, the regression line equation is: $y=19x+1$

2. Calculate the predicted exam scores ($\hat{y}$):

Plug in the values of xx into the regression line equation to get $\hat{y}$.

Calculating:

For $x=2: \hat{y}=19(2)+1=39$
For $x=3: \hat{y}=19(3)+1=58$
For $x=4: \hat{y}=19(4)+1=77$
For $x=5: \hat{y}=19(5)+1=96$
For $x=6: \hat{y}=19(6)+1=115$

So, the predicted exam scores are: 

$\hat{y}=[39,58,77,96,115]$

3. Calculate the residuals ($e_i$):

Subtract the predicted exam scores from the actual exam scores.

Calculating:

For $x=2: e_1=60−39=21$
For $x=3: e_2=70−58=12$
For $x=4: e_3=80−77=3$
For $x=5: e_4=85−96=−11$
For $x=6: e_5=90−115=−25$

So, the residuals are: 

$e=[21,12,3,−11,−25]$

4. Calculate the Mean Squared Error (MSE):

$$MSE = \frac{\sum(y_i- \bar{y})^2}{n}$$
We have calculated $y_i - \bar{r}$ as $e_i$ in the step above.

Calculating:

$$MSE=\frac{(21^2+12^2+3^2+(−11)^2+(−25)^2)}{5}=\frac{(441+144+9+121+625)}{5}= 268$$

So, the Mean Squared Error is MSE = 268

5. Calculate the Root Mean Squared Error (RMSE):

$$RMSE = \sqrt{MSE}$$

Calculating:

$$RMSE=\sqrt{268}≈16.37$$

So, the Root Mean Squared Error is approximately 16.37

6. Calculate the Mean Absolute Error (MAE):

$$MAE = \frac{\sum{|y_i - \bar{y}|}}{n} $$
Once again, we have calculated $y_i - \bar{r}$ as $e_i$ in the step above.

Calculating:

$$MAE=\frac{|21|+|12|+|3|+|-11|+|-25|}{5}=\frac{(21+12+3+11+25)}{5}=\frac{72}{5}=14.4$$
So, the Mean Absolute Error is MAE=14.4.

## Now you.

Suppose we have the following dataset representing the relationship between the number of hours spent studying and the exam scores for a different group of students:

| Hours Studied (x) |	Exam Score (y) |
|---|---|
| 3 |	75 |
| 4 |	80 |
| 6 |	90 |
| 7 |	85 |
| 9 |	95 |

## Model Validation Techniques in R:

### Summary Statistics:

In this example, we will do an exploration of a dataset which includes American women's height and weight. The question we are asking, is can we predict a woman's height given her weight?

```{r}
 # You may need to install the datasets library (by using install.packages('datasets'))
library(datasets)
```

In this example, it's worth plotting the data to get a sense of whether there may be a trend or not. 

```{r}
plot(women, xlab = "Height (in)", ylab = "Weight (lb)",
     main = "women data: American women aged 30-39")
```


The summary() function provides a summary of the regression model, including coefficients, standard errors, and metrics for evaluating the effectiveness of the model.

Example: 
```
summary(lm_model)
```

Below we have created a linear regression model, where we use weight as the dependent variable and height    as the independent variable.

```{r}
lmHeight = lm(height~weight, data = women) #Create the linear regression
summary(lmHeight) #Review the results
```

The most important results are, our Multiple R-Squared and Adjusted R 

### Coefficient of Determination (R-squared):
R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model. Higher values indicate better fit.

If we just want to isolate this statistic, we can call the following function.

Example: 
```
summary(lm_model)$r.squared
```

### Adjusted R-squared:
Adjusted R-squared penalizes the inclusion of unnecessary predictors in the model and provides a more conservative measure of model fit, especially for models with multiple predictors.

If we just want to isolate this statistic, we can call the following function.

Example: 
```
summary(lm_model)$adj.r.squared
```

However, these statistics are included in the summary report given above.

### Root Mean Squared Error (RMSE) or Mean Absolute Error (MAE):
These metrics quantify the average difference between observed and predicted values. Lower values indicate better predictive accuracy.

Example 
```
RMSE: sqrt(mean(residuals(lm_model)^2))
```

```{r}
rmse = sqrt(mean(residuals(lmHeight)^2))
rmse
```


## Multiple Linear Regression

Generally speaking, modelling is going to be concerned with looking at more than one factor. In this case, what we can do is use multiple linear regression to assess whether multiple factors are more useful to predict a value.

In this case, we are looking at predicting a cars price based on some characteristics of the car.
```{r}
automobile <- read.csv("~/automobile.csv")
```

For example, we could look at whether city.mpg is a good predictor of price but doing a simple linear regression model.

```{r}
model = lm(price ~ `city.mpg`, data = automobile) #Create the linear regression
summary(model) #Review the results
```

Not so great with a multiple R squared of 0.4993 and an adjusted R squared of 0.4967. Maybe we can use another variable and see if this helps the model. Lets add highway.mpg. Note, all we do to add another variable is use the + sign and then the next variable.

```{r}
model = lm(price ~ `city.mpg` + `highway.mpg`, data = automobile) #Create the linear regression
summary(model) #Review the results
```

This has improved our multiple R squared to 0.5183 and adjusted R squared to 0.5132. Maybe we can add horsepower to our multiple linear regression model.

```{r}
model = lm(price ~ `city.mpg` + `highway.mpg` + `horsepower`, data = automobile) #Create the linear regression
summary(model) #Review the results
```

We have improved this more dramatically this time, and our multiple R squared value rises to 0.6803 and adjusted R squared value rises to 0.6753.

We have dealt so far with just numerical data, lets see what happens if we use some categorical data by adding make to our linear regression.

```{r}
model = lm(price ~ `city.mpg` + `highway.mpg` + `horsepower` + `make`, data = automobile) #Create the linear regression
summary(model) #Review the results
```

So some things to note with this output. Firstly, our multiple R squared value rises again to 0.9179, and our adjusted R squared value rises to 0.908. But our output looks different. We can see that each make of car is represented, and that some have some * next to their entries, which denotes significance (we can see that these models are a lot better (significant) than others). Before we go any further, lets look at what p values are.

As we know,in a linear regression model, coefficients represent the relationship between the independent variable(s) and the dependent variable. For example, in a simple linear regression y=mx+by=mx+b, 'm' is the coefficient (slope) of the independent variable 'x'.

*Null Hypothesis (H0):* The null hypothesis states that there is no relationship between the independent variable(s) and the dependent variable, i.e., the coefficient(s) equal zero.

*Alternative Hypothesis (H1):* The alternative hypothesis complements the null hypothesis by proposing that there is indeed a relationship between the independent variable(s) and the dependent variable, contrary to the null hypothesis. It suggests that the coefficient(s) in the regression equation are not equal to zero.
        
*P-value:* The p-value associated with each coefficient indicates the probability of observing the coefficient's value, or more extreme, if the null hypothesis is true. In other words, it tells us the likelihood of the observed relationship occurring by random chance. A significant p-value leads to rejecting the null hypothesis in favor of the alternative hypothesis, indicating that there is evidence supporting a relationship between the independent variable(s) and the dependent variable. Conversely, a non-significant p-value suggests that there is insufficient evidence to reject the null hypothesis, indicating that the relationship between the variables is not statistically significant.

*Interpretation:* If the p-value is small (typically less than a chosen significance level, often 0.05), it suggests that the coefficient is statistically significant. In other words, we reject the null hypothesis and conclude that there is evidence of a significant relationship between the independent variable and the dependent variable. If the p-value is large, it suggests that the coefficient is not statistically significant. In this case, we fail to reject the null hypothesis, indicating that there is insufficient evidence to conclude a significant relationship.

*Decision:* Based on the p-value, you decide whether to retain or reject the null hypothesis. If you reject the null hypothesis, it implies that the variable associated with the coefficient in question is indeed contributing significantly to the model's predictive power.

Overall, p-values help researchers determine the validity of their regression models and the significance of the variables included. However, it's essential to interpret p-values in conjunction with other metrics and considerations to make informed decisions about model selection and variable significance.

Now we know what a p value is, we can see that there are certain values which are more useful than others, as indicated by the asterisks (*). Lets check to see if this is right. Remember, we can create a subset of values by using the which command. Lets put all mercedez cars into a smaller dataframe. To compare, lets also put all hondas into a smaller dataframe.

```{r}
# This code takes all rows where make is mercedes-benz and puts it into a new dataframe called mercedes.
mercedes <- automobile[which(automobile$make == 'mercedes-benz'),]
# This code takes all rows where make is honda and puts it into a new dataframe called honda.
honda <- automobile[which(automobile$make == 'honda'),]
```

Now lets run our linear regression model, just for all mercedes cars and all honda cars.

```{r}
model = lm(price ~ `city.mpg` + `highway.mpg` + horsepower, data = mercedes) #Create the linear regression
summary(model) #Review the results
```

```{r}
model = lm(price ~ `city.mpg` + `highway.mpg` + horsepower, data = honda) #Create the linear regression
summary(model) #Review the results
```

We can see that the model is better for mercedes cars 0.9112 and 0.8757, than for honda cars 0.8722 and 0.8297.

### Practical Questions

The main purpose of this week, is to see how we evaluate linear regression models. In our dataset, we have a number of different variables described below.

| Variable | Data |
|----------|------|
| make: | alfa-romero, audi, bmw, chevrolet, dodge, honda, isuzu, jaguar, mazda, mercedes-benz, mercury, mitsubishi, nissan, peugot, plymouth, porsche, renault, saab, subaru, toyota, volkswagen, volvo |
| fuel-type: | diesel, gas. |
| aspiration: | std, turbo. |
| num-of-doors: | four, two. |
| body-style: | hardtop, wagon, sedan, hatchback, convertible. |
| drive-wheels: | 4wd, fwd, rwd. |
| engine-location: | front, rear. |
| wheel-base: | continuous from 86.6 120.9. |
| length: | continuous from 141.1 to 208.1. |
| width: | continuous from 60.3 to 72.3. |
| height: | continuous from 47.8 to 59.8. |
| curb-weight: | continuous from 1488 to 4066. |
| engine-type: | dohc, dohcv, l, ohc, ohcf, ohcv, rotor. |
| num-of-cylinders: | eight, five, four, six, three, twelve, two. |
| engine-size: | continuous from 61 to 326. |
| fuel-system: | 1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi. |
| bore: | continuous from 2.54 to 3.94. |
| stroke: | continuous from 2.07 to 4.17. |
| compression-ratio: | continuous from 7 to 23. |
| horsepower: | continuous from 48 to 288. |
| peak-rpm: | continuous from 4150 to 6600. |
| city-mpg: | continuous from 13 to 49. |
| highway-mpg: | continuous from 16 to 54. |
| price: | continuous from 5118 to 45400. |

Create models to answer the following questions in a R Markdown document.

1. What is the best single variable for predicting price? 
2. Rank, in order, each variable according to Adjusted R squared and also Root Mean Squared. Are the orders different?
3. Is there a combination of variables that improves our model? HINT: Combine the single variables that scored best in various ways to see if you can improve the model.
4. Now that you have found your best combination, answer the following questions:
  1. Does the model perform better for certain makes of car?
  2. Does the model work better for standard or turbo cars?
  3. Does the model work for cars with two or four doors, or is there no difference?


## Conclusion:

Evaluating regression models is essential to determine their accuracy, reliability, and suitability for a given problem.
Metrics like MSE, RMSE, MAE, R², and adjusted R² provide valuable insights into a model's performance.
Model validation techniques and residual analysis help ensure robust and interpretable regression models.
